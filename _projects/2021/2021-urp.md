---
layout:         project_detail
title:          GAN based Sign Language Synthesis Model
start_date:     May
end_date:       Dec 2021
selected:       false
pub:            "Undergraduate Research Program, Seoul National University."
# pub_pre:        "Submitted to "
pub_post:       'advised by Prof. Kyomin Jung.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Commercialized</span>'

abstract: >-
  A GAN-based framework that synthesizes sign language videos by transferring interpreter motions to speakers while preserving identity and facial expressions, enhancing accessibility for deaf audiences.
cover:          /_projects/2021/images/urp_result.png

---

### Problem & Motivation
Traditional sign language interpretation displays interpreters in small corner windows, creating accessibility barriers for deaf viewers who must shift attention between the speaker and interpreter. According to Korea's National Institute of Korean Language, 53% of users cited "small screen size" as the primary barrier to understanding sign language interpretation.


### Architecture
<div style="margin:1em 0; text-align:center;">
<img src = "images/urp_architecture.png" width=600>
<p style="color:#555; font-size:0.95em; margin-top:0.5em;">
Overall architecture of proposed model
</p>
</div>
* **Pose Extraction**: OpenPose library to extract 113 keypoints (54 facial, 50 hand, 9 body landmarks) as skeleton representations
* **Generator**: U-Net architecture with skip connections for detail preservation, taking speaker images and skeleton sequences as input
* **Discriminator**: PatchGAN architecture processing consecutive frame pairs for temporal consistency

### Results
<div style="margin:1em 0; text-align:center;">
<img src = "images/urp_result.png" width=600>
<p style="color:#555; font-size:0.95em; margin-top:0.5em;">
Qualitative results</p>
</div>
Qualitative evaluation showed superior results compared to GestureGAN baseline, with better facial feature preservation and finger detail accuracy. Training convergence was faster due to additional temporal frame information.